import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from arch import arch_model

def _flatten_yf_columns(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.columns, pd.MultiIndex):
        df = df.copy()
        df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]
    return df

def load_spy_for_garch(
    start: str = "2005-01-01",
    end: str | None = None,
    use_log_returns: bool = True,
    scale_returns: float = 100.0,
    test_start: str = "2019-01-01",
):
    df = yf.download(
        "SPY",
        start=start,
        end=end,
        interval="1d",
        auto_adjust=True,
        actions=False,
        progress=False,
    )
    df = _flatten_yf_columns(df)
    if df.empty:
        raise RuntimeError("No data returned from yfinance.")

    df = df[~df.index.duplicated(keep="first")].sort_index()

    px = df["Close"].dropna().copy()
    px.name = "price"

    r = np.log(px).diff() if use_log_returns else px.pct_change()
    r = (r.dropna() * scale_returns).copy()
    r.name = "ret"

    # Realized variance targets aligned to forecast origin t
    s2 = r.pow(2)

    rv1 = s2.shift(-1).copy()
    rv1.name = "rv1"

    rv20 = s2.shift(-1).rolling(20).sum().shift(-19).copy()
    rv20.name = "rv20"

    test_start_ts = pd.Timestamp(test_start)
    train_returns = r.loc[r.index < test_start_ts].copy()
    test_returns = r.loc[r.index >= test_start_ts].copy()

    return {
        "prices": px,
        "returns": r,
        "train_returns": train_returns,
        "test_returns": test_returns,
        "rv1": rv1.dropna(),
        "rv20": rv20.dropna(),
        "raw_df": df,
    }

# Example
data = load_spy_for_garch(start="2005-01-01", test_start="2019-01-01")




def qlike(forecast_var: pd.Series, realized_var: pd.Series, eps: float = 1e-12) -> pd.Series:
    f = forecast_var.clip(lower=eps)
    r = realized_var.clip(lower=eps)
    return (r / f) + np.log(f)


def garch_rolling_oos_forecasts(
    returns: pd.Series,
    origins: pd.DatetimeIndex,
    horizon: int = 20,
    window: int = 252 * 5,           # 5-year rolling window
    mean: str = "Constant",          # "Zero" or "Constant"
    dist: str = "t",                 # "t" or "normal"
    p: int = 1,
    o: int = 0,                      # 0 => GARCH, 1 => GJR-GARCH (leverage)
    q: int = 1,
    rescale: bool = False,           # keep False if you already scaled returns (e.g. *100)
    fit_kwargs: dict | None = None,
) -> pd.DataFrame:
    """
    Rolling-window GARCH forecasts at each origin t:
      - var1  = 1-step ahead conditional variance (t+1 | t)
      - varH  = sum of variances over next `horizon` steps (cumulative horizon variance)
    """
    if fit_kwargs is None:
        fit_kwargs = dict(disp="off", show_warning=False)

    r = returns.dropna().sort_index()

    if len(r) < window + horizon + 5:
        raise ValueError("Not enough data for the requested rolling window and horizon.")

    eps = 1e-12
    out_rows = []

    for t in origins:
        t = pd.Timestamp(t)
        if t not in r.index:
            continue

        loc = r.index.get_loc(t)
        if loc < window - 1:
            continue

        sample = r.iloc[loc - window + 1 : loc + 1]

        am = arch_model(
            sample,
            mean=mean,
            vol="GARCH",
            p=p,
            o=o,
            q=q,
            dist=dist,
            rescale=rescale,
        )
        res = am.fit(**fit_kwargs)

        fcst = res.forecast(horizon=horizon, reindex=False)
        v = fcst.variance.iloc[-1]          # h.1 ... h.H

        var1 = max(float(v.iloc[0]), eps)
        varH = max(float(v.sum()), eps)

        out_rows.append((t, var1, varH))

    out = pd.DataFrame(out_rows, columns=["date", "var1", f"var{horizon}"]).set_index("date")
    return out

# Build evaluation dataset
returns = data["returns"]
rv1 = data["rv1"]
rv20 = data["rv20"]

# Forecast origins: use test dates (or any subset you want) where targets exist
origins = data["test_returns"].index.intersection(rv1.index).intersection(rv20.index)

# Rolling GARCH(1,1)-t forecasts (5y window)
H = 20
fc = garch_rolling_oos_forecasts(
    returns=returns,
    origins=origins,
    horizon=H,
    window=252 * 5,
    mean="Constant",
    dist="t",
    p=1, o=0, q=1
)

eval_df = fc.join(rv1).join(rv20).dropna()

# Optional: also evaluate "average daily variance over next 20 days"
eval_df["var20_avg"] = eval_df[f"var{H}"] / H
eval_df["rv20_avg"] = eval_df["rv20"] / H

# Losses (variance-level)
eval_df["qlike1"] = qlike(eval_df["var1"], eval_df["rv1"])
eval_df["qlike20"] = qlike(eval_df[f"var{H}"], eval_df["rv20"])
eval_df["mse1"] = (eval_df["var1"] - eval_df["rv1"]) ** 2
eval_df["mse20"] = (eval_df[f"var{H}"] - eval_df["rv20"]) ** 2

# Losses (avg-daily variance over 20d) â€” often more interpretable
eval_df["qlike20_avg"] = qlike(eval_df["var20_avg"], eval_df["rv20_avg"])
eval_df["mse20_avg"] = (eval_df["var20_avg"] - eval_df["rv20_avg"]) ** 2

print(eval_df.head())
print("\nAverage losses:")
print(eval_df[["qlike1", "qlike20", "qlike20_avg", "mse1", "mse20", "mse20_avg"]].mean())

# 1-day: forecast vs realized proxy (noisy)
ax = eval_df[["var1", "rv1"]].plot(title="SPY: 1-day variance forecast vs realized proxy")
ax.set_ylabel("Variance (percent^2)")
plt.show()

# 20-day cumulative: forecast vs realized proxy
ax = eval_df[[f"var{H}", "rv20"]].plot(title="SPY: 20-day cumulative variance forecast vs realized proxy")
ax.set_ylabel("Cumulative variance (percent^2)")
plt.show()

# 20-day average daily variance: often easier to read
ax = eval_df[["var20_avg", "rv20_avg"]].plot(title="SPY: 20-day average daily variance (forecast vs realized)")
ax.set_ylabel("Variance (percent^2)")
plt.show()
