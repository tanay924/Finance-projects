import numpy as np
import pandas as pd
import yfinance as yf

def _flatten_yf_columns(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.columns, pd.MultiIndex):
        df = df.copy()
        df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]
    return df

def load_spy_for_garch(
    start: str = "2005-01-01",
    end: str | None = None,
    use_log_returns: bool = True,
    scale_returns: float = 100.0,
    test_start: str = "2019-01-01",
):
    df = yf.download(
        "SPY",
        start=start,
        end=end,
        interval="1d",
        auto_adjust=True,
        actions=False,
        progress=False,
    )
    df = _flatten_yf_columns(df)
    if df.empty:
        raise RuntimeError("No data returned from yfinance.")

    df = df[~df.index.duplicated(keep="first")].sort_index()

    px = df["Close"].dropna().copy()
    px.name = "price"

    r = np.log(px).diff() if use_log_returns else px.pct_change()
    r = (r.dropna() * scale_returns).copy()
    r.name = "ret"

    # Realized variance targets aligned to forecast origin t
    s2 = r.pow(2)

    rv1 = s2.shift(-1).copy()
    rv1.name = "rv1"

    rv20 = s2.shift(-1).rolling(20).sum().shift(-19).copy()
    rv20.name = "rv20"

    test_start_ts = pd.Timestamp(test_start)
    train_returns = r.loc[r.index < test_start_ts].copy()
    test_returns = r.loc[r.index >= test_start_ts].copy()

    return {
        "prices": px,
        "returns": r,
        "train_returns": train_returns,
        "test_returns": test_returns,
        "rv1": rv1.dropna(),
        "rv20": rv20.dropna(),
        "raw_df": df,
    }
# Step 1: Model-free evidence of volatility mean reversion using SPY daily returns (yfinance)
# Assumes you already have `data` from load_spy_for_garch(...) and therefore `data["returns"]`
# pip install statsmodels

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, kpss


def build_realized_vol_proxies(returns: pd.Series, m: int = 20) -> pd.DataFrame:
    """
    returns: daily scaled returns (e.g., 100*log-return), indexed by date.
    m: window length in trading days (e.g., 20 ~ 1 month)

    Produces trailing realized variance RV_m(t) = sum_{i=0..m-1} r_{t-i}^2
    and log-RV plus realized vol.
    """
    r = returns.dropna().sort_index()
    rv = r.pow(2).rolling(m).sum()
    out = pd.DataFrame({
        f"rv{m}": rv,
        f"log_rv{m}": np.log(rv),
        f"rvol{m}": np.sqrt(rv),  # volatility in same units as returns (percent)
    }).dropna()
    return out


def ar1_mean_reversion(log_rv: pd.Series) -> dict:
    """
    Fits AR(1): v_{t} = a + b v_{t-1} + e_t on log realized variance v_t.
    Returns fitted model + implied half-life (in days) if |b|<1 and b>0.
    Uses HAC standard errors (Newey-West).
    """
    v = log_rv.dropna()
    v_lag = v.shift(1).dropna()
    v_now = v.loc[v_lag.index]

    X = sm.add_constant(v_lag)
    ols = sm.OLS(v_now, X).fit(cov_type="HAC", cov_kwds={"maxlags":20})

    a = ols.params["const"]
    b = ols.params[v_lag.name]

    # Half-life for AR(1) (if 0<b<1)
    half_life = None
    if (b > 0) and (abs(b) < 1):
        half_life = float(np.log(0.5) / np.log(b))

    return {"model": ols, "a": float(a), "b": float(b), "half_life_days": half_life}


def pullback_regression(log_rv: pd.Series) -> dict:
    """
    Pull-back regression: Δv_t = c + phi * v_{t-1} + u_t
    Mean reversion corresponds to phi < 0.
    Uses HAC standard errors.
    """
    v = log_rv.dropna()
    dv = v.diff().dropna()
    v_lag = v.shift(1).loc[dv.index]

    X = sm.add_constant(v_lag)
    ols = sm.OLS(dv, X).fit(cov_type="HAC", cov_kwds={"maxlags":20})

    c = ols.params["const"]
    phi = ols.params[v_lag.name]

    return {"model": ols, "c": float(c), "phi": float(phi)}


def stationarity_tests(log_rv: pd.Series) -> dict:
    """
    ADF: H0 = unit root (non-stationary). Low p => reject => stationary.
    KPSS: H0 = stationary. High p => fail to reject => stationary.
    """
    v = log_rv.dropna()

    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(v, autolag="AIC")
    kpss_stat, kpss_p, _, kpss_crit = kpss(v, regression="c", nlags="auto")

    return {
        "adf_stat": adf_stat, "adf_p": adf_p, "adf_crit": adf_crit,
        "kpss_stat": kpss_stat, "kpss_p": kpss_p, "kpss_crit": kpss_crit
    }


r = data["returns"]  # scaled daily returns, e.g. percent log-returns
m = 20

rv_df = build_realized_vol_proxies(r, m=m)
v = rv_df[f"log_rv{m}"]
v.name = f"log_rv{m}"  # ensure a stable name for regression output

# AR(1) on log RV
ar1 = ar1_mean_reversion(v)

# Pull-back regression on log RV
pb = pullback_regression(v)

# Stationarity tests
tests = stationarity_tests(v)

print(f"AR(1) on log RV{m}: v_t = a + b v_(t-1) + e_t")
print(f"  b = {ar1['b']:.6f}")
print(f"  half-life (days) = {ar1['half_life_days']}")
print(ar1["model"].summary())

print(f"\nPull-back: Δv_t = c + phi*v_(t-1) + u_t")
print(f"  phi = {pb['phi']:.6f}  (mean reversion if phi < 0)")
print(pb["model"].summary())

print("\nStationarity tests on log RV:")
print(f"  ADF p-value  = {tests['adf_p']:.6g} (reject unit root if small)")
print(f"  KPSS p-value = {tests['kpss_p']:.6g} (fail to reject stationarity if large)")

ax = rv_df[f"rvol{m}"].plot(title=f"SPY trailing realized volatility: sqrt(RV{m})", figsize=(10,4))
ax.set_ylabel("Vol (percent)")
plt.show()

ax = rv_df[f"log_rv{m}"].plot(title=f"SPY log realized variance: log(RV{m})", figsize=(10,4))
ax.set_ylabel("log variance")
plt.show()

# Optional: scatter to visualize mean reversion (AR(1) slope)
v_lag = v.shift(1)
plt.figure(figsize=(5,5))
plt.scatter(v_lag.dropna(), v.loc[v_lag.dropna().index], s=5)
plt.xlabel(f"log RV{m} (t-1)")
plt.ylabel(f"log RV{m} (t)")
plt.title("Mean reversion visual: AR(1) on log realized variance")
plt.show()
from __future__ import annotations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from arch import arch_model


def qlike(forecast_var: pd.Series, realized_var: pd.Series, eps: float = 1e-12) -> pd.Series:
    f = forecast_var.clip(lower=eps)
    r = realized_var.clip(lower=eps)
    return (r / f) + np.log(f)


def garch_rolling_oos_forecasts(
    returns: pd.Series,
    origins: pd.DatetimeIndex,
    horizon: int = 20,
    window: int = 252 * 5,           # 5-year rolling window
    mean: str = "Constant",          # "Zero" or "Constant"
    dist: str = "t",                 # "t" or "normal"
    p: int = 1,
    o: int = 0,                      # 0 => GARCH, 1 => GJR-GARCH (leverage)
    q: int = 1,
    rescale: bool = False,           # keep False if you already scaled returns (e.g. *100)
    fit_kwargs: dict | None = None,
) -> pd.DataFrame:
    """
    Rolling-window GARCH forecasts at each origin t:
      - var1  = 1-step ahead conditional variance (t+1 | t)
      - varH  = sum of variances over next `horizon` steps (cumulative horizon variance)
    """
    if fit_kwargs is None:
        fit_kwargs = dict(disp="off", show_warning=False)

    r = returns.dropna().sort_index()

    if len(r) < window + horizon + 5:
        raise ValueError("Not enough data for the requested rolling window and horizon.")

    eps = 1e-12
    out_rows = []

    for t in origins:
        t = pd.Timestamp(t)
        if t not in r.index:
            continue

        loc = r.index.get_loc(t)
        if loc < window - 1:
            continue

        sample = r.iloc[loc - window + 1 : loc + 1]

        am = arch_model(
            sample,
            mean=mean,
            vol="GARCH",
            p=p,
            o=o,
            q=q,
            dist=dist,
            rescale=rescale,
        )
        res = am.fit(**fit_kwargs)

        fcst = res.forecast(horizon=horizon, reindex=False)
        v = fcst.variance.iloc[-1]          # h.1 ... h.H

        var1 = max(float(v.iloc[0]), eps)
        varH = max(float(v.sum()), eps)

        out_rows.append((t, var1, varH))

    out = pd.DataFrame(out_rows, columns=["date", "var1", f"var{horizon}"]).set_index("date")
    return out

# Build evaluation dataset
returns = data["returns"]
rv1 = data["rv1"]
rv20 = data["rv20"]

# Forecast origins: use test dates (or any subset you want) where targets exist
origins = data["test_returns"].index.intersection(rv1.index).intersection(rv20.index)

# Rolling GARCH(1,1)-t forecasts (5y window)
H = 20
fc = garch_rolling_oos_forecasts(
    returns=returns,
    origins=origins,
    horizon=H,
    window=252 * 5,
    mean="Constant",
    dist="t",
    p=1, o=0, q=1
)

eval_df = fc.join(rv1).join(rv20).dropna()

# Optional: also evaluate "average daily variance over next 20 days"
eval_df["var20_avg"] = eval_df[f"var{H}"] / H
eval_df["rv20_avg"] = eval_df["rv20"] / H

# Losses (variance-level)
eval_df["qlike1"] = qlike(eval_df["var1"], eval_df["rv1"])
eval_df["qlike20"] = qlike(eval_df[f"var{H}"], eval_df["rv20"])
eval_df["mse1"] = (eval_df["var1"] - eval_df["rv1"]) ** 2
eval_df["mse20"] = (eval_df[f"var{H}"] - eval_df["rv20"]) ** 2

# Losses (avg-daily variance over 20d) — often more interpretable
eval_df["qlike20_avg"] = qlike(eval_df["var20_avg"], eval_df["rv20_avg"])
eval_df["mse20_avg"] = (eval_df["var20_avg"] - eval_df["rv20_avg"]) ** 2

print(eval_df.head())
print("\nAverage losses:")
print(eval_df[["qlike1", "qlike20", "qlike20_avg", "mse1", "mse20", "mse20_avg"]].mean())

# 1-day: forecast vs realized proxy (noisy)
ax = eval_df[["var1", "rv1"]].plot(title="SPY: 1-day variance forecast vs realized proxy")
ax.set_ylabel("Variance (percent^2)")
plt.show()

# 20-day cumulative: forecast vs realized proxy
ax = eval_df[[f"var{H}", "rv20"]].plot(title="SPY: 20-day cumulative variance forecast vs realized proxy")
ax.set_ylabel("Cumulative variance (percent^2)")
plt.show()

# 20-day average daily variance: often easier to read
ax = eval_df[["var20_avg", "rv20_avg"]].plot(title="SPY: 20-day average daily variance (forecast vs realized)")
ax.set_ylabel("Variance (percent^2)")
plt.show()

# -------------------------
# Optional: try GJR-GARCH (leverage) and compare
# -------------------------
fc_gjr = garch_rolling_oos_forecasts(
    returns=returns,
    origins=origins,
    horizon=H,
    window=252 * 5,
    mean="Constant",
    dist="t",
    p=1, o=1, q=1    # GJR-GARCH(1,1)
)

eval_gjr = fc_gjr.join(rv1).join(rv20).dropna()
eval_gjr["var20_avg"] = eval_gjr[f"var{H}"] / H
eval_gjr["rv20_avg"] = eval_gjr["rv20"] / H
eval_gjr["qlike1"] = qlike(eval_gjr["var1"], eval_gjr["rv1"])
eval_gjr["qlike20"] = qlike(eval_gjr[f"var{H}"], eval_gjr["rv20"])
eval_gjr["qlike20_avg"] = qlike(eval_gjr["var20_avg"], eval_gjr["rv20_avg"])

print("\nQLIKE comparison (lower is better):")
print(pd.DataFrame({
    "GARCH_t": eval_df[["qlike1", "qlike20", "qlike20_avg"]].mean(),
    "GJR_t":   eval_gjr[["qlike1", "qlike20", "qlike20_avg"]].mean(),
}))
